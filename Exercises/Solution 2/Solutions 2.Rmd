---
title: "Tilastollinen Oppimisen Jatkokurssi Round 2"
output: html_notebook
---

## Exercise 1

*Varian: Replicate classification and regression trees and pruning in connection to Titanic example. (Notice that in 1. and 2. exercises, seemingly the obtained results are not necessarily exactly the same as reported in Varian (2014). Report and interpret your findings.)*

First we import the dataset, drop unusable columns, remove NAs and create testing and training sets with a 20:80 testing:training split. For modelling pruposes we also need to convert survived to a factor.
```{r}
set.seed(1309)
titanic3 = read.csv("titanic3.csv")
columns_to_keep = c(1, 2, 4, 5, 6, 7, 9, 11)
titanic3 = titanic3[ , columns_to_keep]
titanic3 = na.omit(titanic3)
titanic3$survived = as.factor(titanic3$survived)
train_index = sample(seq(1045), 836)
train = titanic3[train_index, ]
test = titanic3[-train_index, ]
```
Now we start modelling using classification trees from the rpart-library. The first model will be a large model with at least 10 elements in each leaf. For the splitting criterion the Gini index was used chosen instead of cross-entropy, they should give similar results since they are numerically similar but the Gini index has an interpretation as the variance (ESL).
```{r}
library(rpart)
library(rpart.plot)
set.seed(1309)
titanic.model.0 = rpart(survived ~ ., train, method = "class", parms = list(split = "gini"), control = rpart.control(minsplit = 30, minbucket = 10, cp = 0.001, maxdepth = 5, xval = 10))
rpart.plot(titanic.model.0, type = 5, extra = 2, under = TRUE)
```
Now we need to prune the large model using complexity pruning. Fortunately, rpart already performs k-fold crossvalidation on the $\alpha$ (Cp) parameter. We can view the results from the crossvalidation using and then select the best one. Based on the table 0.0072046 or 0.0019212 seem equally good while based on the plot 0.013 or 0.0037 seem equally good, there seems to be a discrepancy between the values reported by the printcp and plotcp commands. The nsplit and size of the tree can however be used to see that 0.0072046 corresponds to 0.013 and 0.0019212 corresponds to 0.0037. The best choice will generally be the largest choice if many $\alpha$s have similar errors. For now lets choose 0.0072047 which will give us the tree of size 6.
```{r}
printcp(titanic.model.0)
plotcp(titanic.model.0)
titanic.model.pruned = prune(titanic.model.0, cp = 0.0072047)
rpart.plot(titanic.model.pruned, type = 5, extra = 2, under = TRUE)
```
Next we confirm that our modelling method is indeed performing as expected by lookint at the accuracies of all the different models.
```{r}
pred.titanic.pruned.test = predict(titanic.model.pruned, newdata = test, type = "class")
pred.titanic.pruned.train = predict(titanic.model.pruned, type = "class")
pred.titanic.0.test = predict(titanic.model.0, newdata = test, type = "class")
pred.titanic.0.train = predict(titanic.model.0, type = "class")
confusion.titanic.pruned.test = table(pred.titanic.pruned.test, test$survived)
confusion.titanic.pruned.train = table(pred.titanic.pruned.train, train$survived)
confusion.titanic.0.test = table(pred.titanic.0.test, test$survived)
confusion.titanic.0.train = table(pred.titanic.0.train, train$survived)
accuracy.titanic.0.train = (confusion.titanic.0.train[1, 1] + confusion.titanic.0.train[2, 2])/sum(confusion.titanic.0.train)
accuracy.titanic.0.test = (confusion.titanic.0.test[1, 1] + confusion.titanic.0.test[2, 2])/sum(confusion.titanic.0.test)
accuracy.titanic.pruned.train = (confusion.titanic.pruned.train[1, 1] + confusion.titanic.pruned.train[2, 2])/sum(confusion.titanic.pruned.train)
accuracy.titanic.pruned.test = (confusion.titanic.pruned.test[1, 1] + confusion.titanic.pruned.test[2, 2])/sum(confusion.titanic.pruned.test)
prop.table(confusion.titanic.0.train); accuracy.titanic.0.train
prop.table(confusion.titanic.0.test); accuracy.titanic.0.test
prop.table(confusion.titanic.pruned.train); accuracy.titanic.pruned.train
prop.table(confusion.titanic.pruned.test); accuracy.titanic.pruned.test
```
It seems like everything has worked quite well, overfitting was reduced since the full model training accuracy is higher than the pruned models training accuracy and this helped generalization since the pruned models testing accuracy was higher than the full models testing accuracy.

Using the pruned model as the final model, that is using the model below:
```{r}
rpart.plot(titanic.model.pruned, type = 4, extra = 2, under = TRUE)
```
We see that the best predictor for survival is the sex of the person, where males older than 9.5 die in 399 out of 491 cases. For males younger than 9.5 surviving is largely decided by how many siblings you have on the boat, if you are fewer than 3 siblings you have a big chance of surviving.

Females seem to have an almost as high chance of surviving as males have of dying, for them the next important predictor is whether they are of a sufficiently high class or not. For females in the 3rd class cabins the chance of survival seems rouglhy 50:50 with a high chance of survival if they embarked in Cherbourg (realtively few people so maybe a tight knight group helping each other?).

## Exercise 2
*Varian: Replicate the example of Home Mortgage Disclosure Act (HMDA) data. In particular, see and reproduce Figure 5 (interpret your findings).*

This time we'll be using the party libary instead of rpart. We load the data from the Ecdat-library, note the discrepancy in spelling (HMDA/HDMA). We also plit into training and testing sets (80:20 split).
```{r}
library(Ecdat)
library(party)
data(Hdma)
head(Hdma)
Hdma = na.omit(Hdma)
set.seed(2380)
test_index = sample(seq(2380), size = 476)
hmda.test = Hdma[test_index, ]
hmda.train = Hdma[-test_index, ]
```
Next we use ctree to build a large classification tree.
```{r}
hmda.0 = ctree(deny ~ ., hmda.train, controls = ctree_control(teststat = "quad", testtype = "Bonferroni", mincriterion = 0.95, minsplit = 30, minbucket = 10))
plot(hmda.0)
```
Since party doesn't perform k-fold crossvalidation on the mincriterion variable (the variable that determines if we should split or not) we will have to do it ourselves.
```{r}
set.seed(2030)
meanerror = vector(mode = "double", length = 50)
meanerrorvar = vector(mode = "double", length = 50)
grid = exp(seq(-10, 0, length = 50))
#grid[1] = (grid[1] + grid[2])/2
folds = sample(cut(seq(1, 1904), breaks = 10, labels = FALSE), 1904)
for (i in 1:50) {
    errors = vector(mode = "double", length = 10)
    for (j in 1:10) {
        cvtestsetindex = which(folds == j, arr.ind = TRUE)
        cvfoldtestset = hmda.train[cvtestsetindex, ]
        cvfoldtrainset = hmda.train[-cvtestsetindex, ]
        tmpmodel = ctree(deny ~ ., cvfoldtrainset, controls = ctree_control(teststat = "quad", testtype = "Bonferroni", mincriterion = 1 - grid[i], minsplit = 30, minbucket = 10))
        tmppred = predict(tmpmodel, newdata = cvfoldtestset, type = "response")
        conf = table(tmppred, cvfoldtestset$deny)
        errors[j] = (conf[1, 2] + conf[2, 1])/sum(conf)
    }
    meanerror[i] = mean(errors)
    meanerrorvar[i] = var(errors)
}
sdev = sqrt(meanerrorvar)/sqrt(1714)
plot(x = log(grid), y = meanerror, ylim = c(0.05, 0.15))
#arrows(log(grid), meanerror - sdev, log(grid), meanerror + sdev, length=0.025, angle=90, code = 3)
abline(a = min(meanerror) + sdev[which.min(meanerror)], b = 0)
```
Crossvalidation suggests choosing the smallest of the values we tried, which gives a value as closeto 1 as possible for mincriterion (mincriterion = 1 - the p-value used to decide whether to include a split or not, the p-value needs to be larger than 1 - mincriterion). That yields the following model:
```{r}
hmda.pruned = ctree(deny ~ ., hmda.train, controls = ctree_control(teststat = "quad", testtype = "Bonferroni", mincriterion = 1 - grid[which.min(meanerror)], minsplit = 30, minbucket = 10))
plot(hmda.pruned)
```
Since black is not a factor in any of the models we've looked at the kind of analysis that Varian did (repeat but without black in the dataset) is unnecessary. We can however again look at the confusion matrices and accuracies to see if choosing the mincriterion using crossvalidation gave us an increase in test accuracy.
```{r}
hmda.train.pred.0 = predict(hmda.0, newdata = hmda.train, type = "response")
hmda.test.pred.0 = predict(hmda.0, newdata = hmda.test, type = "response")
hmda.train.pred.pruned = predict(hmda.pruned, newdata = hmda.train, type = "response")
hmda.test.pred.pruned = predict(hmda.pruned, newdata = hmda.test, type = "response")
conf.hmda.train.0 = table(hmda.train.pred.0, hmda.train$deny)
conf.hmda.test.0 = table(hmda.test.pred.0, hmda.test$deny)
conf.hmda.train.pruned = table(hmda.train.pred.pruned, hmda.train$deny)
conf.hmda.test.pruned = table(hmda.test.pred.pruned, hmda.test$deny)
acc.hmda.train.0 = (conf.hmda.train.0[1, 1] + conf.hmda.train.0[2, 2])/sum(conf.hmda.train.0)
acc.hmda.test.0 = (conf.hmda.test.0[1, 1] + conf.hmda.test.0[2, 2])/sum(conf.hmda.test.0)
acc.hmda.train.pruned = (conf.hmda.train.pruned[1, 1] + conf.hmda.train.pruned[2, 2])/sum(conf.hmda.train.pruned)
acc.hmda.test.pruned = (conf.hmda.test.pruned[1, 1] + conf.hmda.test.pruned[2, 2])/sum(conf.hmda.test.pruned)
conf.hmda.train.0; prop.table(conf.hmda.train.0); acc.hmda.train.0
conf.hmda.test.0; prop.table(conf.hmda.test.0); acc.hmda.test.0
conf.hmda.train.pruned; prop.table(conf.hmda.train.pruned); acc.hmda.train.pruned
conf.hmda.test.pruned; prop.table(conf.hmda.test.pruned); acc.hmda.test.pruned
```
This time it seems like we did not get an increase in test accuracy but that might have been expected judging by the plot of 1 - mincriterion versus error above. The accuracies seem pretty close but one thing of concern is the large amount of false positives, i.e. the model guesses yes too often, even when the answer is no. This might be because the dataset is a bit unbalanced with many more granted loans than denied loans.

The most important predictor is if the person was denied mortgage insurance, where a yes gives a high chance of being denied. If the consumer was granted mortgage insurance the next most important predictor is their consumer credit score. A consumer credit score higher (higher equals worse) than 2 with a public bad credit record gives a roughly 50:50 chance of being denied, the same is true if their consumer credit score is lower than 2 and if more than 50% of their income goes towards debt payments. Otherwise they have a very high chance of getting a loan.

## Exercise 3
*In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.*

#### (a)
*Split the data set into a training set and a test set.*

Import data from the ISLR library and attach the Carseats-dataset. Split into training and test sets with a 80:20 split.
```{r}
library(ISLR)
attach(Carseats)
cs = Carseats
set.seed(400)
test_index = sample(seq(400), 80)
cs.test = cs[test_index, ]
cs.trai = cs[-test_index, ]
```
#### (b)
*Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?*

In order to predict Sales we will fit a largeish regression tree on the training set using rpart and plot it using rpart.plot.
```{r}
library(rpart)
library(rpart.plot)
cs.0 = rpart(Sales ~ ., data = cs.trai, method = "anova", control = rpart.control(minsplit = 10, minbucket = 3, cp = 0.01, xval = 10))
cs.0.test.predict = predict(cs.0, newdata = cs.test)
cs.0.trai.predict = predict(cs.0, newdata = cs.trai)
cs.0.test.mse = mean((cs.0.test.predict - cs.test$Sales)^2)
cs.0.trai.mse = mean((cs.0.trai.predict - cs.trai$Sales)^2)
cs.0.test.mse; cs.0.trai.mse; rpart.plot(cs.0, type = 4, extra = 101, under = TRUE)
```
For this large tree the most important variable was ShelveLoc, followed by price. After this advertisment, age, shelve location (again) and prices (again) play a role. Further if the shelve location is Bad or Medium and the price is over 106 the tree gets very deep indicating that there is high variability there. For good shelve locations the tree is relatively short indicating lower vairability in sales.

The test-MSE is 4.753288 while the train-MSE is 2.343768, perhaps we are overfitting?

#### (c)
*Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?*

The rpart library does 10-fold crossvalidation by default so we just need to interpret the results and choose a model.
```{r}
printcp(cs.0); plotcp(cs.0)
cs.pruned = prune(cs.0, cp = (cs.0$cptable[which.min(cs.0$cptable[, 4]), 1]))
rpart.plot(cs.pruned, type = 4, extra = 101, under = TRUE)
```
10-fold crossvalidation suggested a treesize of 6 (5 splits), this leaves more than 20 items per leaf (5% of all items) which seems reasonable. The best selling items seem to be the (relatively) cheap ones in a good shelve location and the ones that might be at a bad shelve location but are much cheaper than the competitors pricing.

Next we compute the test-MSE to see if the pruning improved anything else than 
interpretability.
```{r}
cs.pruned.test.predict = predict(cs.pruned, newdata = cs.test)
cs.pruned.trai.predict = predict(cs.pruned, newdata = cs.trai)
cs.pruned.test.mse = mean((cs.pruned.test.predict - cs.test$Sales)^2)
cs.pruned.trai.mse = mean((cs.pruned.trai.predict - cs.trai$Sales)^2)
cs.pruned.test.mse; cs.pruned.trai.mse
```
This time it seems like we took a slight hit in test-MSE, with the large tree the test-MSE was 4.753288 while the pruned model had a test-MSE of 5.041223, not too big of a drop, certainly compensated for with the increase in interpretability.

## Exercise 4
*This problem involves the OJ data set which is part of the ISLR package.*

Loading data.
```{r}
library(ISLR)
oj = data.frame(OJ)
```
#### (a)
*Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.*

Remove NA:s, sample random indexes.
```{r}
set.seed(1070)
oj = na.omit(oj)
len = length(oj[, 1])
ts_index = sample(seq(len), len - 800)
oj.test = oj[ts_index, ]
oj.trai = oj[-ts_index, ]
```
#### (b)
*Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?*

Purchase is a factor-variable, we will be fitting a classification tree using the Gini index as a split-criterion.
```{r}
oj.0 = rpart(Purchase ~ ., data = oj.trai, method = "class", parms = list(split = "gini"), control = rpart.control(maxsurrogate = 0, maxcompete = 0))
summary(oj.0)
```
Interpreting the summary, it seems like the most important variable is LoyalCH, it being almost 10 times as important as PriceDiff. The third most important variable seems to be ListPriceDiff. It seems like the first split is quite even, ~500 going left and ~300 going right. At the right we already have a terminal node or leaf, predicting a MM for Purchased. This node (node 3) gives 70 misclassifications. To the left from the first split we have another split (node 2), again splitting on LoyalCH. This second split is again pretty even with ~300 going left and ~200 going right. To the left (node 4) we have our second terminal node, again predicting CH, this time with only 13 misclassifications. The third split (node 5) is more uneven with ~140 going left (node 10) and ~70 going right (node 11). Again we have a terminal node to the left (node 10), predicting CH with 25 misclassifications. To the right (node 11) we have an uneven split with ~20 going left (node 22) and ~50 going right (node 23). To the left we classify as CH and to the right as MM with 7 and 14 misclassifications respectively.

Summing over all the misclassifications in the 5 terminal nodes and dividing by 800 we get the training error rate:
```{r}
(70+13+25+7+14)/800
```

#### (c)
*Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.*

```{r}
oj.0
```
Picking node 23 as the terminal node and following from the top we see that this is a node where LoyalCH is between 0.482935 and 0.705699 while the PriceDiff is smaller than 0.015. Finally the ListPriceDiff is smaller than 0.235 and we predict MM. We misclassify 14 out of 46 items.

#### (d)
*Create a plot of the tree, and interpret the results.*

```{r}
rpart.plot(oj.0, type = 4, extra = 2, under = TRUE)
```
Well it looks like our previous interpretation was correct but the plot is certainly a lot nicer to look at.

#### (e)
*Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?*

```{r}
oj.0.pred.test = predict(oj.0, newdata = oj.test, type = "class")
oj.0.conf.test = table(oj.0.pred.test, oj.test$Purchase)
oj.0.erro.test = (oj.0.conf.test[1, 2] + oj.0.conf.test[2, 1]) / sum(oj.0.conf.test)
oj.0.erro.test
```

#### (f)
*Apply the cv.tree() function to the training set in order to determine the optimal tree size.*

Since rpart does 10-fold crossvalidation as part of the initial modelling we will use the already computed values to determine the optimal tree size:
```{r}
printcp(oj.0); print("Optimal treesize:") ;oj.0$cptable[which.min(oj.0$cptable[, 4]), 2] + 1
```

#### (g)
*Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.*

rpart has a method for this but the error rate is rescaled in relation to the error rate of the root node (0.385 in this case). Also the x-axis is the cp variable while the alternative x-axis is the treesize:
```{r}
plotcp(oj.0)
```
Instead we can create our own error plot:
```{r}
plot(x = oj.0$cptable[, 2] + 1, y = oj.0$cptable[, 4]*0.385, type = "l", ylim = c(0,1))
```
A disadvantage is that this plot doesn't have the min + standard error line that the documentation suggests to use for selecting the optimal treesize/cp. In our case this method suggest using a tree of size 2.

#### (h)
*Which tree size corresponds to the lowest cross-validated classification error rate?*

A treesize of 5 corresponds to the lowest cross-validated classification error rate.

## Exercise 5
*Continued from previous*

#### (i)
*Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.*

Since the previously mentioned method suggests a tree size of 2 and while the "smallest crossvalidation error" method suggests our original tree, which happens to also be of size 5, we prune to a treesize of 2:
```{r}
oj.p = prune(oj.0, cp = 0.1)
```

#### (j)
*Compare the training error rates between the pruned and unpruned trees. Which is higher?*

```{r}
oj.p.pred.trai = predict(oj.p, newdata = oj.trai, type = "class")
oj.0.pred.trai = predict(oj.0, newdata = oj.trai, type = "class")
oj.p.conf.trai = table(oj.p.pred.trai, oj.trai$Purchase)
oj.0.conf.trai = table(oj.0.pred.trai, oj.trai$Purchase)
oj.p.erro.trai = (oj.p.conf.trai[1, 2] + oj.p.conf.trai[2, 1]) / sum(oj.p.conf.trai)
oj.0.erro.trai = (oj.0.conf.trai[1, 2] + oj.0.conf.trai[2, 1]) / sum(oj.0.conf.trai)
print("Unpruned training error:"); oj.0.erro.trai; print("Pruned training error:"); oj.p.erro.trai
```
The pruned model naturally has a higher trainign set error rate.

#### (k)
*Compare the test error rates between the pruned and unpruned trees. Which is higher?*

```{r}
oj.p.pred.test = predict(oj.p, newdata = oj.test, type = "class")
oj.0.pred.test = predict(oj.0, newdata = oj.test, type = "class")
oj.p.conf.test = table(oj.p.pred.test, oj.test$Purchase)
oj.0.conf.test = table(oj.0.pred.test, oj.test$Purchase)
oj.p.erro.test = (oj.p.conf.test[1, 2] + oj.p.conf.test[2, 1]) / sum(oj.p.conf.test)
oj.0.erro.test = (oj.0.conf.test[1, 2] + oj.0.conf.test[2, 1]) / sum(oj.0.conf.test)
print("Unpruned testing error:"); oj.0.erro.test; print("Pruned testing error:"); oj.p.erro.test
```

Again, the pruned model has a higher testing error which is also perhaps suggested since the exercise hints that this might be the optimal model :)