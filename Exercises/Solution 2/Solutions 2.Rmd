---
title: "Tilastollinen Oppimisen Jatkokurssi Round 2"
output: html_notebook
---

## Exercise 1

*Varian: Replicate classification and regression trees and pruning in connection to Titanic example. (Notice that in 1. and 2. exercises, seemingly the obtained results are not necessarily exactly the same as reported in Varian (2014). Report and interpret your findings.)*

First we import the dataset, drop unusable columns, remove NAs and create testing and training sets with a 20:80 testing:training split. For modelling pruposes we also need to convert survived to a factor.
```{r}
set.seed(1309)
titanic3 = read.csv("titanic3.csv")
columns_to_keep = c(1, 2, 4, 5, 6, 7, 9, 11)
titanic3 = titanic3[ , columns_to_keep]
titanic3 = na.omit(titanic3)
titanic3$survived = as.factor(titanic3$survived)
train_index = sample(seq(1045), 836)
train = titanic3[train_index, ]
test = titanic3[-train_index, ]
```
Now we start modelling using classification trees from the rpart-library. The first model will be a large model with at least 10 elements in each leaf. For the splitting criterion the Gini index was used chosen instead of cross-entropy, they should give similar results since they are numerically similar but the Gini index has an interpretation as the variance (ESL).
```{r}
library(rpart)
library(rpart.plot)
set.seed(1309)
titanic.model.0 = rpart(survived ~ ., train, method = "class", parms = list(split = "gini"), control = rpart.control(minsplit = 30, minbucket = 10, cp = 0.001, maxdepth = 5, xval = 10))
rpart.plot(titanic.model.0, type = 5, extra = 2, under = TRUE)
```
Now we need to prune the large model using complexity pruning. Fortunately, rpart already performs k-fold crossvalidation on the $\alpha$ (Cp) parameter. We can view the results from the crossvalidation using and then select the best one. Based on the table 0.0072046 or 0.0019212 seem equally good while based on the plot 0.013 or 0.0037 seem equally good, there seems to be a discrepancy between the values reported by the printcp and plotcp commands. The nsplit and size of the tree can however be used to see that 0.0072046 corresponds to 0.013 and 0.0019212 corresponds to 0.0037. The best choice will generally be the largest choice if many $\alpha$s have similar errors. For now lets choose 0.0072047 which will give us the tree of size 6.
```{r}
printcp(titanic.model.0)
plotcp(titanic.model.0)
titanic.model.pruned = prune(titanic.model.0, cp = 0.0072047)
rpart.plot(titanic.model.pruned, type = 5, extra = 2, under = TRUE)
```
Next we confirm that our modelling method is indeed performing as expected by lookint at the accuracies of all the different models.
```{r}
pred.titanic.pruned.test = predict(titanic.model.pruned, newdata = test, type = "class")
pred.titanic.pruned.train = predict(titanic.model.pruned, type = "class")
pred.titanic.0.test = predict(titanic.model.0, newdata = test, type = "class")
pred.titanic.0.train = predict(titanic.model.0, type = "class")
confusion.titanic.pruned.test = table(pred.titanic.pruned.test, test$survived)
confusion.titanic.pruned.train = table(pred.titanic.pruned.train, train$survived)
confusion.titanic.0.test = table(pred.titanic.0.test, test$survived)
confusion.titanic.0.train = table(pred.titanic.0.train, train$survived)
accuracy.titanic.0.train = (confusion.titanic.0.train[1, 1] + confusion.titanic.0.train[2, 2])/sum(confusion.titanic.0.train)
accuracy.titanic.0.test = (confusion.titanic.0.test[1, 1] + confusion.titanic.0.test[2, 2])/sum(confusion.titanic.0.test)
accuracy.titanic.pruned.train = (confusion.titanic.pruned.train[1, 1] + confusion.titanic.pruned.train[2, 2])/sum(confusion.titanic.pruned.train)
accuracy.titanic.pruned.test = (confusion.titanic.pruned.test[1, 1] + confusion.titanic.pruned.test[2, 2])/sum(confusion.titanic.pruned.test)
prop.table(confusion.titanic.0.train); accuracy.titanic.0.train
prop.table(confusion.titanic.0.test); accuracy.titanic.0.test
prop.table(confusion.titanic.pruned.train); accuracy.titanic.pruned.train
prop.table(confusion.titanic.pruned.test); accuracy.titanic.pruned.test
```
It seems like everything has worked quite well, overfitting was reduced since the full model training accuracy is higher than the pruned models training accuracy and this helped generalization since the pruned models testing accuracy was higher than the full models testing accuracy.

Using the pruned model as the final model, that is using the model below:
```{r}
rpart.plot(titanic.model.pruned, type = 4, extra = 2, under = TRUE)
```
We see that the best predictor for survival is the sex of the person, where males older than 9.5 die in 399 out of 491 cases. For males younger than 9.5 surviving is largely decided by how many siblings you have on the boat, if you are fewer than 3 siblings you have a big chance of surviving.

Females seem to have an almost as high chance of surviving as males have of dying, for them the next important predictor is whether they are of a sufficiently high class or not. For females in the 3rd class cabins the chance of survival seems rouglhy 50:50 with a high chance of survival if they embarked in Cherbourg (realtively few people so maybe a tight knight group helping each other?).

## Exercise 2
*Varian: Replicate the example of Home Mortgage Disclosure Act (HMDA) data. In particular, see and reproduce Figure 5 (interpret your findings).*

This time we'll be using the party libary instead of rpart. We load the data from the Ecdat-library, note the discrepancy in spelling (HMDA/HDMA). We also plit into training and testing sets (80:20 split).
```{r}
library(Ecdat)
library(party)
data(Hdma)
head(Hdma)
Hdma = na.omit(Hdma)
set.seed(2380)
test_index = sample(seq(2380), size = 476)
hmda.test = Hdma[test_index, ]
hmda.train = Hdma[-test_index, ]
```
Next we use ctree to build a large classification tree.
```{r}
hmda.0 = ctree(deny ~ ., hmda.train, controls = ctree_control(teststat = "quad", testtype = "Bonferroni", mincriterion = 0.95, minsplit = 30, minbucket = 10))
plot(hmda.0)
```
Since party doesn't perform k-fold crossvalidation on the mincriterion variable (the variable that determines if we should split or not) we will have to do it ourselves.
```{r}
set.seed(2030)
meanerror = vector(mode = "double", length = 50)
meanerrorvar = vector(mode = "double", length = 50)
grid = exp(seq(-10, 0, length = 50))
#grid[1] = (grid[1] + grid[2])/2
folds = sample(cut(seq(1, 1904), breaks = 10, labels = FALSE), 1904)
for (i in 1:50) {
    errors = vector(mode = "double", length = 10)
    for (j in 1:10) {
        cvtestsetindex = which(folds == j, arr.ind = TRUE)
        cvfoldtestset = hmda.train[cvtestsetindex, ]
        cvfoldtrainset = hmda.train[-cvtestsetindex, ]
        tmpmodel = ctree(deny ~ ., cvfoldtrainset, controls = ctree_control(teststat = "quad", testtype = "Bonferroni", mincriterion = 1 - grid[i], minsplit = 30, minbucket = 10))
        tmppred = predict(tmpmodel, newdata = cvfoldtestset, type = "response")
        conf = table(tmppred, cvfoldtestset$deny)
        errors[j] = (conf[1, 2] + conf[2, 1])/sum(conf)
    }
    meanerror[i] = mean(errors)
    meanerrorvar[i] = var(errors)
}
sdev = sqrt(meanerrorvar)/sqrt(1714)
plot(x = log(grid), y = meanerror, ylim = c(0.05, 0.15))
#arrows(log(grid), meanerror - sdev, log(grid), meanerror + sdev, length=0.025, angle=90, code = 3)
abline(a = min(meanerror) + sdev[which.min(meanerror)], b = 0)
```
Crossvalidation suggests choosing the smallest of the values we tried, which gives a value as closeto 1 as possible for mincriterion (mincriterion = 1 - the p-value used to decide whether to include a split or not, the p-value needs to be larger than 1 - mincriterion). That yields the following model:
```{r}
hmda.pruned = ctree(deny ~ ., hmda.train, controls = ctree_control(teststat = "quad", testtype = "Bonferroni", mincriterion = 1 - grid[which.min(meanerror)], minsplit = 30, minbucket = 10))
plot(hmda.pruned)
```
Since black is not a factor in any of the models we've looked at the kind of analysis that Varian did (repeat but without black in the dataset) is unnecessary. We can however again look at the confusion matrices and accuracies to see if choosing the mincriterion using crossvalidation gave us an increase in test accuracy.
```{r}
hmda.train.pred.0 = predict(hmda.0, newdata = hmda.train, type = "response")
hmda.test.pred.0 = predict(hmda.0, newdata = hmda.test, type = "response")
hmda.train.pred.pruned = predict(hmda.pruned, newdata = hmda.train, type = "response")
hmda.test.pred.pruned = predict(hmda.pruned, newdata = hmda.test, type = "response")
conf.hmda.train.0 = table(hmda.train.pred.0, hmda.train$deny)
conf.hmda.test.0 = table(hmda.test.pred.0, hmda.test$deny)
conf.hmda.train.pruned = table(hmda.train.pred.pruned, hmda.train$deny)
conf.hmda.test.pruned = table(hmda.test.pred.pruned, hmda.test$deny)
acc.hmda.train.0 = (conf.hmda.train.0[1, 1] + conf.hmda.train.0[2, 2])/sum(conf.hmda.train.0)
acc.hmda.test.0 = (conf.hmda.test.0[1, 1] + conf.hmda.test.0[2, 2])/sum(conf.hmda.test.0)
acc.hmda.train.pruned = (conf.hmda.train.pruned[1, 1] + conf.hmda.train.pruned[2, 2])/sum(conf.hmda.train.pruned)
acc.hmda.test.pruned = (conf.hmda.test.pruned[1, 1] + conf.hmda.test.pruned[2, 2])/sum(conf.hmda.test.pruned)
conf.hmda.train.0; prop.table(conf.hmda.train.0); acc.hmda.train.0
conf.hmda.test.0; prop.table(conf.hmda.test.0); acc.hmda.test.0
conf.hmda.train.pruned; prop.table(conf.hmda.train.pruned); acc.hmda.train.pruned
conf.hmda.test.pruned; prop.table(conf.hmda.test.pruned); acc.hmda.test.pruned
```
This time it seems like we did not get an increase in test accuracy but that might have been expected judging by the plot of 1 - mincriterion versus error above. The accuracies seem pretty close but one thing of concern is the large amount of false positives, i.e. the model guesses yes too often, even when the answer is no. This might be because the dataset is a bit unbalanced with many more granted loans than denied loans.

The most important predictor is if the person was denied mortgage insurance, where a yes gives a high chance of being denied. If the consumer was granted mortgage insurance the next most important predictor is their consumer credit score. A consumer credit score higher (higher equals worse) than 2 with a public bad credit record gives a roughly 50:50 chance of being denied, the same is true if their consumer credit score is lower than 2 and if more than 50% of their income goes towards debt payments. Otherwise they have a very high chance of getting a loan.

## Exercise 3